{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome on beezy.sh Welcome to beezy.sh - the Rom's content/memory/tinkery repositories Who's Rom LinkedIn Profile Open Transformation Leader at Red Hat - Present Open Source Strategy Advisor at Ondat - Present Founder & Maintainer of the Trousseau.io project - GitRepo Maintainer of the [Discoblocks.io](https://discoblocks.io project - GitRepo Portfolio of contributions, articles and webinars Contributions Digital Wallonia, a proposal for a digital plan (in French) - Belgian Walloon Regional Government Sept 2015 Articles The Intrepid Squirrel is Out! - Ondat.io Dec 2021 3 key takeaways from KubeCon NA 2021 - Ondat.io Nov 2021 Stateful Application in Kubernetes: it pays to plan ahead - The New Stack Oct 2021 On-premises vs cloud native storage - The New Stack Aug 2021 The biggest gap in Kubernetes storage architecture - The New Stack Jul 2021 How to solve kubernetes persistent storage challenges - The New Stack Jun 2021 Deploying cloud native persistent storage in the age of containers - The New Stack May 2021 Live Presentations Open 22 - Open Source to Enterprise Open Source - Kangaroot Mar 2022 - Antwerpen Red Hat Academy - Innovation with Open Source - Henallux Mar 2022 - Namur Ansible Automates BeLux 2022 - Master of Ceremony - Red Hat May 2022 - Brussels Open for Gov - Refactoring your application at your own pace - Kangaroot Jun 2022 - Brussels HashiConf - A Kubernetes Zero-Trust Security Model using a KMS provider plugin - HashiCorp Europe Jun 2022 - Amsterdam HashiCorp User Group - Solving the Kubernetes Secret conundrum - Meetup @ Natilik Jun 2022 - London Webinars Trousseau - the Kubernetes Key Management Service provider - Fosdem Feb 2022 (waiting on video availability) How to keep a Kubernetes secret a secret the universal way - CNCF on-demand webinar Dec 2021 Cloud Field Day 12 - Tech Field Day Nov 2021 Solving Data Protection the DevOps way - DevOps.com Sep 2021 How to solve Kubernetes Persistent Storage at scale and speed the GitOps way - DeveloperWeek Global Cloud Sep 2021 Accelerating Kubernetes Onboarding and Application Transformation - StorageOS on-demand webinar Aug 2021 Database-as-a-Service with PostgreSQL and persistent storage - StorageOS on-demand webinar May 2021 We have been hacked! - Red Hat Tech Month Feb 2021 Secrets or not, but don't clear text - Fosdem Feb 2021 From remote with love - Red Hat EMEA Forum Nov 2020","title":"Welcome on beezy.sh"},{"location":"#welcome-on-beezysh","text":"Welcome to beezy.sh - the Rom's content/memory/tinkery repositories","title":"Welcome on beezy.sh"},{"location":"#whos-rom","text":"LinkedIn Profile Open Transformation Leader at Red Hat - Present Open Source Strategy Advisor at Ondat - Present Founder & Maintainer of the Trousseau.io project - GitRepo Maintainer of the [Discoblocks.io](https://discoblocks.io project - GitRepo","title":"Who's Rom"},{"location":"#portfolio-of-contributions-articles-and-webinars","text":"","title":"Portfolio of contributions, articles and webinars"},{"location":"#contributions","text":"Digital Wallonia, a proposal for a digital plan (in French) - Belgian Walloon Regional Government Sept 2015","title":"Contributions"},{"location":"#articles","text":"The Intrepid Squirrel is Out! - Ondat.io Dec 2021 3 key takeaways from KubeCon NA 2021 - Ondat.io Nov 2021 Stateful Application in Kubernetes: it pays to plan ahead - The New Stack Oct 2021 On-premises vs cloud native storage - The New Stack Aug 2021 The biggest gap in Kubernetes storage architecture - The New Stack Jul 2021 How to solve kubernetes persistent storage challenges - The New Stack Jun 2021 Deploying cloud native persistent storage in the age of containers - The New Stack May 2021","title":"Articles"},{"location":"#live-presentations","text":"Open 22 - Open Source to Enterprise Open Source - Kangaroot Mar 2022 - Antwerpen Red Hat Academy - Innovation with Open Source - Henallux Mar 2022 - Namur Ansible Automates BeLux 2022 - Master of Ceremony - Red Hat May 2022 - Brussels Open for Gov - Refactoring your application at your own pace - Kangaroot Jun 2022 - Brussels HashiConf - A Kubernetes Zero-Trust Security Model using a KMS provider plugin - HashiCorp Europe Jun 2022 - Amsterdam HashiCorp User Group - Solving the Kubernetes Secret conundrum - Meetup @ Natilik Jun 2022 - London","title":"Live Presentations"},{"location":"#webinars","text":"Trousseau - the Kubernetes Key Management Service provider - Fosdem Feb 2022 (waiting on video availability) How to keep a Kubernetes secret a secret the universal way - CNCF on-demand webinar Dec 2021 Cloud Field Day 12 - Tech Field Day Nov 2021 Solving Data Protection the DevOps way - DevOps.com Sep 2021 How to solve Kubernetes Persistent Storage at scale and speed the GitOps way - DeveloperWeek Global Cloud Sep 2021 Accelerating Kubernetes Onboarding and Application Transformation - StorageOS on-demand webinar Aug 2021 Database-as-a-Service with PostgreSQL and persistent storage - StorageOS on-demand webinar May 2021 We have been hacked! - Red Hat Tech Month Feb 2021 Secrets or not, but don't clear text - Fosdem Feb 2021 From remote with love - Red Hat EMEA Forum Nov 2020","title":"Webinars"},{"location":"hashiconf/","text":"HashiConf Europe 2022 A Kubernetes ZeroTrust Model using a KMS provider plugin Session: Wednesday at 11:15 AM \u00b7 15 min \u00b7 Hallway Track @ Westerliefde Abstract Kubernetes is dominating cloud native application deployment along with the re-platforming and refactoring of legacy applications with the support of HashiCorp Vault. However, critical low level Kubernetes components like CNI, CSI, Operators are not capable or design to benefits from the power of HashiCorp Vault. The usage of Kubernetes KMS provider plugin solves these challenges by leveraging HahiCorp Vault as the One to rule them all enforcing a Zero-Trust security model.","title":"HashiConf Europe 2022"},{"location":"hashiconf/#hashiconf-europe-2022","text":"","title":"HashiConf Europe 2022"},{"location":"hashiconf/#a-kubernetes-zerotrust-model-using-a-kms-provider-plugin","text":"Session: Wednesday at 11:15 AM \u00b7 15 min \u00b7 Hallway Track @ Westerliefde","title":"A Kubernetes ZeroTrust Model using a KMS provider plugin"},{"location":"hashiconf/#abstract","text":"Kubernetes is dominating cloud native application deployment along with the re-platforming and refactoring of legacy applications with the support of HashiCorp Vault. However, critical low level Kubernetes components like CNI, CSI, Operators are not capable or design to benefits from the power of HashiCorp Vault. The usage of Kubernetes KMS provider plugin solves these challenges by leveraging HahiCorp Vault as the One to rule them all enforcing a Zero-Trust security model.","title":"Abstract"},{"location":"trousseau/","text":"Trousseau Sequence Diagram Kubernetes & Vault Configuration sequenceDiagram participant k8s participant vault autonumber k8s->>vault: ServiceAccount for Kubernetes Auth k8s->>k8s: RBAC rules Note right of vault: Enable Transit Engine Note right of vault: Create Transit Key Note right of vault: Create Transit policy Note right of vault: Create Token linked to policy Note right of vault: Create Key-Value storing config vault->>k8s: Bind vault config to Trousseau Specs Trousseau Deployment sequenceDiagram participant k8s participant trousseau participant vault autonumber Note over k8s,trousseau: Deployment k8s->>trousseau: Apply Trousseau DaemonSet Note right of trousseau: Vault Agent Sidecar trousseau->>vault: Recover Trousseau ConfigMap config vault->>trousseau: Inject Transit Engine Key config loop Healthcheck trousseau->>vault: validate Vault connection end Trousseau Operations sequenceDiagram participant k8s participant trousseau participant vault autonumber Note over k8s,vault: Encryption Operation k8s->>trousseau: kube-manager sent Secret for Encryption trousseau->>vault: Encrypt Secret payload with Transit Key trousseau->>k8s: Send encrypted payload to kube-manager Note over k8s,vault: Decryption Operation k8s->>trousseau: kube-manager sent Secret for Decryption trousseau->>vault: Decrypt Secret payload with Transit Key trousseau->>k8s: Send decrypted payload to kube-manager Vault Token Renewal sequenceDiagram participant k8s participant trousseau participant vault autonumber Note over k8s,vault: Token Rotation vault->>vault: Renew Token Note right of vault: Update Key-Value config k8s->>trousseau: Delete all Trousseau POD Note right of trousseau: Vault Agent Sidecar trousseau->>vault: Recover Trousseau ConfigMap config vault->>trousseau: Inject Transit Engine Key config loop Healthcheck trousseau->>vault: validate Vault connection end Dear Bill, Renewing the Vault token used by Trousseau to access to the Transit Key Engine is to be considered as a day 2 operation from a vault perspective which requires to have access to the root token or a high privileged token to renew the related token. Considering the above, from a security perspective, we can't include this day 2 operation within Trousseau code as this would result in a poor security model and a breach regarding the separation of duties. The below steps will address the token renewal day 2 operation. Since the max TTL is capped to 768h, the renewal of the token is not possible as it would exceed that parameter. To address this renewal cap, a new token needs to be created and inject within the config key-value store. On Vault, create a new token [root@tdevhvc-01 trousseau-demo]# vault token create -policy=trousseau-transit-ro Key Value --- ----- token s.Vhc1rXyveyc4Vn8Upd2M1g9H token_accessor RTk81akLCLOy2JDyzuAQawDk token_duration 768h token_renewable true token_policies [\"default\" \"trousseau-transit-ro\"] identity_policies [] policies [\"default\" \"trousseau-transit-ro\"] On Vault, check the Trousseau config key-value store: [root@tdevhvc-01 trousseau-demo]# vault kv get /secret/trousseau/config ======= Metadata ======= Key Value --- ----- created_time 2022-05-18T00:25:03.155739032Z custom_metadata <nil> deletion_time n/a destroyed false version 1 ========= Data ========= Key Value --- ----- transitkeyname trousseau-kms-vault ttl 30s vaultaddress http://tdevhvc-01.trousseau.io:8200 vaulttoken s.CkUWvzQamSIiRRzYiY8Jibfy On Vault, update the Trousseau config key-value store: [root@tdevhvc-01 trousseau-demo]# vault kv put /secret/trousseau/config transitkeyname=trousseau-kms-vault ttl=30s vaultaddress=http://tdevhvc-01.trousseau.io:8200 vaulttoken=s.Vhc1rXyveyc4Vn8Upd2M1g9H Key Value --- ----- created_time 2022-06-05T15:31:39.881547821Z custom_metadata <nil> deletion_time n/a destroyed false version 2 On Vault, verify the Trousseau config key-value store: [root@tdevhvc-01 trousseau-demo]# vault kv get /secret/trousseau/config ======= Metadata ======= Key Value --- ----- created_time 2022-06-05T15:31:39.881547821Z custom_metadata <nil> deletion_time n/a destroyed false version 2 ========= Data ========= Key Value --- ----- transitkeyname trousseau-kms-vault ttl 30s vaultaddress http://tdevhvc-01.trousseau.io:8200 vaulttoken s.Vhc1rXyveyc4Vn8Upd2M1g9H On the Kubernetes cluster, restart the Trousseau DaemonSet Pods: [root@tdevk8s-01 ~]# kubectl rollout restart -n kube-system daemonset vault-kms-provider daemonset.apps/vault-kms-provider restarted This last part re-initiate the redeployment of the Pods by starting the Vault Agent to fetch the configuration file which includes the new token. Vault agent started! Log data will stream in below:Sun, Jun 5 2022 11:52:18 pmSun, Jun 5 2022 11:52:18 pm==> Vault agent configuration:Sun, Jun 5 2022 11:52:18 pmSun, Jun 5 2022 11:52:18 pmCgo: disabledSun, Jun 5 2022 11:52:18 pmLog Level: debugSun, Jun 5 2022 11:52:18 pmVersion: Vault v1.9.4Sun, Jun 5 2022 11:52:18 pmVersion Sha: fcbe948b2542a13ee8036ad07dd8ebf8554f56cbSun, Jun 5 2022 11:52:18 pm2022-06-05T15:52:18.127Z [INFO] sink.file: creating file sinkSun, Jun 5 2022 11:52:18 pm2022-06-05T15:52:18.127Z [INFO] sink.file: file sink configured: path=/home/vault/.vault-token mode=-rw-r-----Sun, Jun 5 2022 11:52:18 pmSun, Jun 5 2022 11:52:18 pm2022-06-05T15:52:18.127Z [INFO] sink.server: starting sink serverSun, Jun 5 2022 11:52:18 pm2022-06-05T15:52:18.127Z [INFO] template.server: starting template serverSun, Jun 5 2022 11:52:18 pm2022-06-05T15:52:18.127Z [INFO] (runner) creating new runner (dry: false, once: false)Sun, Jun 5 2022 11:52:18 pm2022-06-05T15:52:18.127Z [INFO] auth.handler: starting auth handlerSun, Jun 5 2022 11:52:18 pm2022-06-05T15:52:18.127Z [INFO] auth.handler: authenticatingSun, Jun 5 2022 11:52:18 pm2022-06-05T15:52:18.128Z [DEBUG] (runner) final config: {\"Consul\":{\"Address\":\"\",\"Namespace\":\"\",\"Auth\":{\"Enabled\":false,\"Username\":\"\",\"Password\":\"\"},\"Retry\":{\"Attempts\":12,\"Backoff\":250000000,\"MaxBackoff\":60000000000,\"Enabled\":true},\"SSL\":{\"CaCert\":\"\",\"CaPath\":\"\",\"Cert\":\"\",\"Enabled\":false,\"Key\":\"\",\"ServerName\":\"\",\"Verify\":true},\"Token\":\"\",\"Transport\":{\"CustomDialer\":null,\"DialKeepAlive\":30000000000,\"DialTimeout\":30000000000,\"DisableKeepAlives\":false,\"IdleConnTimeout\":90000000000,\"MaxIdleConns\":100,\"MaxIdleConnsPerHost\":9,\"TLSHandshakeTimeout\":10000000000}},\"Dedup\":{\"Enabled\":false,\"MaxStale\":2000000000,\"Prefix\":\"consul-template/dedup/\",\"TTL\":15000000000,\"BlockQueryWaitTime\":60000000000},\"DefaultDelims\":{\"Left\":null,\"Right\":null},\"Exec\":{\"Command\":\"\",\"Enabled\":false,\"Env\":{\"Denylist\":[],\"Custom\":[],\"Pristine\":false,\"Allowlist\":[]},\"KillSignal\":2,\"KillTimeout\":30000000000,\"ReloadSignal\":null,\"Splay\":0,\"Timeout\":0},\"KillSignal\":2,\"LogLevel\":\"DEBUG\",\"MaxStale\":2000000000,\"PidFile\":\"\",\"ReloadSignal\":1,\"Syslog\":{\"Enabled\":false,\"Facility\":\"LOCAL0\",\"Name\":\"consul-template\"},\"Templates\":[{\"Backup\":false,\"Command\":\"\",\"CommandTimeout\":30000000000,\"Contents\":\"{{- with secret \\\"secret/data/trousseau/config\\\" }}\\n--- \\nprovider: vault\\nvault:\\n keynames:\\n - {{ .Data.data.transitkeyname }} \\n address: {{ .Data.data.vaultaddress }} \\n token: {{ .Data.data.vaulttoken }} \\n{{ end }} \\n\",\"CreateDestDirs\":true,\"Destination\":\"/etc/secrets/config.yaml\",\"ErrMissingKey\":false,\"Exec\":{\"Command\":\"\",\"Enabled\":false,\"Env\":{\"Denylist\":[],\"Custom\":[],\"Pristine\":false,\"Allowlist\":[]},\"KillSignal\":2,\"KillTimeout\":30000000000,\"ReloadSignal\":null,\"Splay\":0,\"Timeout\":30000000000},\"Perms\":0,\"Source\":\"\",\"Wait\":{\"Enabled\":false,\"Min\":0,\"Max\":0},\"LeftDelim\":\"\",\"RightDelim\":\"\",\"FunctionDenylist\":[],\"SandboxPath\":\"\"}],\"Vault\":{\"Address\":\"https://spk-vault.cloudsams.edb.local:8200\",\"Enabled\":true,\"Namespace\":\"\",\"RenewToken\":false,\"Retry\":{\"Attempts\":12,\"Backoff\":250000000,\"MaxBackoff\":60000000000,\"Enabled\":true},\"SSL\":{\"CaCert\":\"\",\"CaPath\":\"\",\"Cert\":\"\",\"Enabled\":true,\"Key\":\"\",\"ServerName\":\"\",\"Verify\":true},\"Transport\":{\"CustomDialer\":null,\"DialKeepAlive\":30000000000,\"DialTimeout\":30000000000,\"DisableKeepAlives\":false,\"IdleConnTimeout\":90000000000,\"MaxIdleConns\":100,\"MaxIdleConnsPerHost\":9,\"TLSHandshakeTimeout\":10000000000},\"UnwrapToken\":false,\"DefaultLeaseDuration\":300000000000},\"Wait\":{\"Enabled\":false,\"Min\":0,\"Max\":0},\"Once\":false,\"BlockQueryWaitTime\":60000000000}Sun, Jun 5 2022 11:52:18 pm2022-06-05T15:52:18.128Z [INFO] (runner) creating watcherSun, Jun 5 2022 11:53:18 pm2022-06-05T15:53:18.128Z [ERROR] auth.handler: error authenticating: error=\"context deadline exceeded\" backoff=1sSun, Jun 5 2022 11:53:19 pm2022-06-05T15:53:19.128Z [INFO] auth.handler: authenticatingSun, Jun 5 2022 11:54:19 pm2022-06-05T15:54:19.129Z [ERROR] auth.handler: error authenticating: error=\"context deadline exceeded\" backoff=1.76sSun, Jun 5 2022 11:54:20 pm2022-06-05T15:54:20.889Z [INFO] auth.handler: authenticatingSun, Jun 5 2022 11:55:20 pm2022-06-05T15:55:20.892Z [ERROR] auth.handler: error authenticating: error=\"context deadline exceeded\" backoff=3.17sSun, Jun 5 2022 11:55:24 pm2022-06-05T15:55:24.071Z [INFO] auth.handler: authenticatingSun, Jun 5 2022 11:56:24 pm2022-06-05T15:56:24.071Z [ERROR] auth.handler: error authenticating: error=\"context deadline exceeded\" backoff=5.51sSun, Jun 5 2022 11:56:29 pm2022-06-05T15:56:29.585Z [INFO] auth.handler: authenticating","title":"Trousseau"},{"location":"trousseau/#trousseau","text":"","title":"Trousseau"},{"location":"trousseau/#sequence-diagram","text":"","title":"Sequence Diagram"},{"location":"trousseau/#kubernetes-vault-configuration","text":"sequenceDiagram participant k8s participant vault autonumber k8s->>vault: ServiceAccount for Kubernetes Auth k8s->>k8s: RBAC rules Note right of vault: Enable Transit Engine Note right of vault: Create Transit Key Note right of vault: Create Transit policy Note right of vault: Create Token linked to policy Note right of vault: Create Key-Value storing config vault->>k8s: Bind vault config to Trousseau Specs","title":"Kubernetes &amp; Vault Configuration"},{"location":"trousseau/#trousseau-deployment","text":"sequenceDiagram participant k8s participant trousseau participant vault autonumber Note over k8s,trousseau: Deployment k8s->>trousseau: Apply Trousseau DaemonSet Note right of trousseau: Vault Agent Sidecar trousseau->>vault: Recover Trousseau ConfigMap config vault->>trousseau: Inject Transit Engine Key config loop Healthcheck trousseau->>vault: validate Vault connection end","title":"Trousseau Deployment"},{"location":"trousseau/#trousseau-operations","text":"sequenceDiagram participant k8s participant trousseau participant vault autonumber Note over k8s,vault: Encryption Operation k8s->>trousseau: kube-manager sent Secret for Encryption trousseau->>vault: Encrypt Secret payload with Transit Key trousseau->>k8s: Send encrypted payload to kube-manager Note over k8s,vault: Decryption Operation k8s->>trousseau: kube-manager sent Secret for Decryption trousseau->>vault: Decrypt Secret payload with Transit Key trousseau->>k8s: Send decrypted payload to kube-manager","title":"Trousseau Operations"},{"location":"trousseau/#vault-token-renewal","text":"sequenceDiagram participant k8s participant trousseau participant vault autonumber Note over k8s,vault: Token Rotation vault->>vault: Renew Token Note right of vault: Update Key-Value config k8s->>trousseau: Delete all Trousseau POD Note right of trousseau: Vault Agent Sidecar trousseau->>vault: Recover Trousseau ConfigMap config vault->>trousseau: Inject Transit Engine Key config loop Healthcheck trousseau->>vault: validate Vault connection end Dear Bill, Renewing the Vault token used by Trousseau to access to the Transit Key Engine is to be considered as a day 2 operation from a vault perspective which requires to have access to the root token or a high privileged token to renew the related token. Considering the above, from a security perspective, we can't include this day 2 operation within Trousseau code as this would result in a poor security model and a breach regarding the separation of duties. The below steps will address the token renewal day 2 operation. Since the max TTL is capped to 768h, the renewal of the token is not possible as it would exceed that parameter. To address this renewal cap, a new token needs to be created and inject within the config key-value store. On Vault, create a new token [root@tdevhvc-01 trousseau-demo]# vault token create -policy=trousseau-transit-ro Key Value --- ----- token s.Vhc1rXyveyc4Vn8Upd2M1g9H token_accessor RTk81akLCLOy2JDyzuAQawDk token_duration 768h token_renewable true token_policies [\"default\" \"trousseau-transit-ro\"] identity_policies [] policies [\"default\" \"trousseau-transit-ro\"] On Vault, check the Trousseau config key-value store: [root@tdevhvc-01 trousseau-demo]# vault kv get /secret/trousseau/config ======= Metadata ======= Key Value --- ----- created_time 2022-05-18T00:25:03.155739032Z custom_metadata <nil> deletion_time n/a destroyed false version 1 ========= Data ========= Key Value --- ----- transitkeyname trousseau-kms-vault ttl 30s vaultaddress http://tdevhvc-01.trousseau.io:8200 vaulttoken s.CkUWvzQamSIiRRzYiY8Jibfy On Vault, update the Trousseau config key-value store: [root@tdevhvc-01 trousseau-demo]# vault kv put /secret/trousseau/config transitkeyname=trousseau-kms-vault ttl=30s vaultaddress=http://tdevhvc-01.trousseau.io:8200 vaulttoken=s.Vhc1rXyveyc4Vn8Upd2M1g9H Key Value --- ----- created_time 2022-06-05T15:31:39.881547821Z custom_metadata <nil> deletion_time n/a destroyed false version 2 On Vault, verify the Trousseau config key-value store: [root@tdevhvc-01 trousseau-demo]# vault kv get /secret/trousseau/config ======= Metadata ======= Key Value --- ----- created_time 2022-06-05T15:31:39.881547821Z custom_metadata <nil> deletion_time n/a destroyed false version 2 ========= Data ========= Key Value --- ----- transitkeyname trousseau-kms-vault ttl 30s vaultaddress http://tdevhvc-01.trousseau.io:8200 vaulttoken s.Vhc1rXyveyc4Vn8Upd2M1g9H On the Kubernetes cluster, restart the Trousseau DaemonSet Pods: [root@tdevk8s-01 ~]# kubectl rollout restart -n kube-system daemonset vault-kms-provider daemonset.apps/vault-kms-provider restarted This last part re-initiate the redeployment of the Pods by starting the Vault Agent to fetch the configuration file which includes the new token. Vault agent started! Log data will stream in below:Sun, Jun 5 2022 11:52:18 pmSun, Jun 5 2022 11:52:18 pm==> Vault agent configuration:Sun, Jun 5 2022 11:52:18 pmSun, Jun 5 2022 11:52:18 pmCgo: disabledSun, Jun 5 2022 11:52:18 pmLog Level: debugSun, Jun 5 2022 11:52:18 pmVersion: Vault v1.9.4Sun, Jun 5 2022 11:52:18 pmVersion Sha: fcbe948b2542a13ee8036ad07dd8ebf8554f56cbSun, Jun 5 2022 11:52:18 pm2022-06-05T15:52:18.127Z [INFO] sink.file: creating file sinkSun, Jun 5 2022 11:52:18 pm2022-06-05T15:52:18.127Z [INFO] sink.file: file sink configured: path=/home/vault/.vault-token mode=-rw-r-----Sun, Jun 5 2022 11:52:18 pmSun, Jun 5 2022 11:52:18 pm2022-06-05T15:52:18.127Z [INFO] sink.server: starting sink serverSun, Jun 5 2022 11:52:18 pm2022-06-05T15:52:18.127Z [INFO] template.server: starting template serverSun, Jun 5 2022 11:52:18 pm2022-06-05T15:52:18.127Z [INFO] (runner) creating new runner (dry: false, once: false)Sun, Jun 5 2022 11:52:18 pm2022-06-05T15:52:18.127Z [INFO] auth.handler: starting auth handlerSun, Jun 5 2022 11:52:18 pm2022-06-05T15:52:18.127Z [INFO] auth.handler: authenticatingSun, Jun 5 2022 11:52:18 pm2022-06-05T15:52:18.128Z [DEBUG] (runner) final config: {\"Consul\":{\"Address\":\"\",\"Namespace\":\"\",\"Auth\":{\"Enabled\":false,\"Username\":\"\",\"Password\":\"\"},\"Retry\":{\"Attempts\":12,\"Backoff\":250000000,\"MaxBackoff\":60000000000,\"Enabled\":true},\"SSL\":{\"CaCert\":\"\",\"CaPath\":\"\",\"Cert\":\"\",\"Enabled\":false,\"Key\":\"\",\"ServerName\":\"\",\"Verify\":true},\"Token\":\"\",\"Transport\":{\"CustomDialer\":null,\"DialKeepAlive\":30000000000,\"DialTimeout\":30000000000,\"DisableKeepAlives\":false,\"IdleConnTimeout\":90000000000,\"MaxIdleConns\":100,\"MaxIdleConnsPerHost\":9,\"TLSHandshakeTimeout\":10000000000}},\"Dedup\":{\"Enabled\":false,\"MaxStale\":2000000000,\"Prefix\":\"consul-template/dedup/\",\"TTL\":15000000000,\"BlockQueryWaitTime\":60000000000},\"DefaultDelims\":{\"Left\":null,\"Right\":null},\"Exec\":{\"Command\":\"\",\"Enabled\":false,\"Env\":{\"Denylist\":[],\"Custom\":[],\"Pristine\":false,\"Allowlist\":[]},\"KillSignal\":2,\"KillTimeout\":30000000000,\"ReloadSignal\":null,\"Splay\":0,\"Timeout\":0},\"KillSignal\":2,\"LogLevel\":\"DEBUG\",\"MaxStale\":2000000000,\"PidFile\":\"\",\"ReloadSignal\":1,\"Syslog\":{\"Enabled\":false,\"Facility\":\"LOCAL0\",\"Name\":\"consul-template\"},\"Templates\":[{\"Backup\":false,\"Command\":\"\",\"CommandTimeout\":30000000000,\"Contents\":\"{{- with secret \\\"secret/data/trousseau/config\\\" }}\\n--- \\nprovider: vault\\nvault:\\n keynames:\\n - {{ .Data.data.transitkeyname }} \\n address: {{ .Data.data.vaultaddress }} \\n token: {{ .Data.data.vaulttoken }} \\n{{ end }} \\n\",\"CreateDestDirs\":true,\"Destination\":\"/etc/secrets/config.yaml\",\"ErrMissingKey\":false,\"Exec\":{\"Command\":\"\",\"Enabled\":false,\"Env\":{\"Denylist\":[],\"Custom\":[],\"Pristine\":false,\"Allowlist\":[]},\"KillSignal\":2,\"KillTimeout\":30000000000,\"ReloadSignal\":null,\"Splay\":0,\"Timeout\":30000000000},\"Perms\":0,\"Source\":\"\",\"Wait\":{\"Enabled\":false,\"Min\":0,\"Max\":0},\"LeftDelim\":\"\",\"RightDelim\":\"\",\"FunctionDenylist\":[],\"SandboxPath\":\"\"}],\"Vault\":{\"Address\":\"https://spk-vault.cloudsams.edb.local:8200\",\"Enabled\":true,\"Namespace\":\"\",\"RenewToken\":false,\"Retry\":{\"Attempts\":12,\"Backoff\":250000000,\"MaxBackoff\":60000000000,\"Enabled\":true},\"SSL\":{\"CaCert\":\"\",\"CaPath\":\"\",\"Cert\":\"\",\"Enabled\":true,\"Key\":\"\",\"ServerName\":\"\",\"Verify\":true},\"Transport\":{\"CustomDialer\":null,\"DialKeepAlive\":30000000000,\"DialTimeout\":30000000000,\"DisableKeepAlives\":false,\"IdleConnTimeout\":90000000000,\"MaxIdleConns\":100,\"MaxIdleConnsPerHost\":9,\"TLSHandshakeTimeout\":10000000000},\"UnwrapToken\":false,\"DefaultLeaseDuration\":300000000000},\"Wait\":{\"Enabled\":false,\"Min\":0,\"Max\":0},\"Once\":false,\"BlockQueryWaitTime\":60000000000}Sun, Jun 5 2022 11:52:18 pm2022-06-05T15:52:18.128Z [INFO] (runner) creating watcherSun, Jun 5 2022 11:53:18 pm2022-06-05T15:53:18.128Z [ERROR] auth.handler: error authenticating: error=\"context deadline exceeded\" backoff=1sSun, Jun 5 2022 11:53:19 pm2022-06-05T15:53:19.128Z [INFO] auth.handler: authenticatingSun, Jun 5 2022 11:54:19 pm2022-06-05T15:54:19.129Z [ERROR] auth.handler: error authenticating: error=\"context deadline exceeded\" backoff=1.76sSun, Jun 5 2022 11:54:20 pm2022-06-05T15:54:20.889Z [INFO] auth.handler: authenticatingSun, Jun 5 2022 11:55:20 pm2022-06-05T15:55:20.892Z [ERROR] auth.handler: error authenticating: error=\"context deadline exceeded\" backoff=3.17sSun, Jun 5 2022 11:55:24 pm2022-06-05T15:55:24.071Z [INFO] auth.handler: authenticatingSun, Jun 5 2022 11:56:24 pm2022-06-05T15:56:24.071Z [ERROR] auth.handler: error authenticating: error=\"context deadline exceeded\" backoff=5.51sSun, Jun 5 2022 11:56:29 pm2022-06-05T15:56:29.585Z [INFO] auth.handler: authenticating","title":"Vault Token Renewal"},{"location":"articles/3_key_takeaways_from_kubecon_na_2021/","text":"","title":"3 key takeaways from kubecon na 2021"},{"location":"articles/the_intrepid_squirrel_is_out/","text":"The Intrepid Squirrel is Out! Source: Ondat.io - Date: 8/12/2021 - Author: Romuald Vandepoel The Marketing and Product teams gave me a unique opportunity to write a blog post about the General Availability of Ondat 2.5. This is quite an honor and I will use it to raise my case regarding the usage of cool code names! Today, we are proud to announce the General Availability of 'Intrepid Squirrel' running on the 2.5 branch of our Ondat core product. Our Intrepid Squirrel is the result of our incredible Engineering teams, your work has been crucial in achieving this important milestone! You rock! Releasing at this time of the year is perfect to provide features and improvements that help with the holiday season operational freeze. Our little adventurous friend gathered the best treats to relieve our customers not only from the seasonal challenges but from the general operational burden of running cloud and on-prem Kubernetes clusters. What's in the tree dens? Our little friend's home is overflowing with good stuff! Here are my three favorite ones: \"I am Tank. I'll be your operator\" It always pays to plan ahead and our Intrepid Squirrel knows it! This is the reason why our Operator has been entirely rewritten to pave the road to the future. Kubernetes is a fast-moving project with continuous improvements at every layer including the persistent storage one. The new operator provides our customer with a frictionless deployment and maintainability strategy, an agile adoption of new Kubernetes primitives while avoiding the deprecation traps, and an open door to new Ondat products! Happy Kubecuddle! Exploring and maintaining a new solution is not always as simple as it looks despite well-written documentation. That's why our adventurous squirrel decided to streamline the installation and operational processes by introducing a kubectl plugin to deploy and operate one or many Ondat data mesh clusters. Let's have a couple of examples: Always good to have some preflight checks before having the \"hum!\" moment: kubectl storageos preflight Screenshot 2021-12-08 at 18.46.26 What about deploying a self-evaluation or test environment with one easy command: kubectl storageos install --include-etcd --admin-username rom --admin-password mysecretpassword namespace/storageos-etcd created kubectl get pod -n storageos-etcd NAME READY STATUS RESTARTS AGE storageos-etcd-0-qcq8h 1/1 Running 0 2m3s storageos-etcd-1-d7lbv 1/1 Running 0 2m3s storageos-etcd-2-p72tm 1/1 Running 0 2m3sstorageos-etcd-controller-manager-7c6df47dfb-5xlq5 1/1 Running 0 2m15s storageos-etcd-proxy-64cf4f6556-dnfrg 1/1 Running 0 2m15s kubectl get pod -n storageos NAME READY STATUS RESTARTS AGE storageos-api-manager-65f5c9dbdf-p4c92 1/1 Running 1 (52s ago) 61s storageos-api-manager-65f5c9dbdf-wdg59 1/1 Running 0 61s storageos-csi-helper-65dc8ff9d8-hmz4q 3/3 Running 0 61s storageos-node-6z4zd 3/3 Running 0 103s storageos-node-bvmrq 3/3 Running 0 103s storageos-node-czbp9 3/3 Running 0 103s storageos-node-fsqms 3/3 Running 0 103s storageos-node-tmrm8 3/3 Running 0 103s storageos-node-vbmml 3/3 Running 0 103s storageos-operator-b7f6ff69-qrkwv 2/2 Running 0 2m17sstorageos-scheduler-6f86686756-wpvzf 1/1 Running 0 109s Done! And there are more to look at... but your turn now! Replicas... replicas everywhere! Whilst the cloud can abstract and simplify many infrastructure challenges, running a Kubernetes data plane in only one availability zone will result in a major disruption in event of a human or provider failure. Well in this release, our Intrepid Squirrel decided to extend his data home range with more than one availability zone, by detecting them with the native Kubernetes topology key or using our own custom key. Allowing the system to then provision the primary volumes and replicas evenly across these availability zones. In the event of a zone failure, our Rapid Recovery will elect one of the replicas as primary volume, call on the Kubernetes scheduler to restart the StatefulSet to the new zone, and start a new replica to honor the required replica count. Also with some clever optimizations, our Intrepid Squirrel got faster, way faster, up to 45 times faster when provisioning new replicas and rejoining a volume group by leveraging concurrent data replications across multiple zones and optimizing the network resources. Let's verify the available regions, create a PVC, scale the number of replicas to 2 (1 primary volume + 2 replicas = 3 zones), then check where are the volume and its replicas placed, hopefully in different availability zones: kubectl get node -ojson |jq '.items[].metadata.labels'|grep \"topology.kubernetes.io/zone\" \"topology.kubernetes.io/zone\": \"us-central1-f\" \"topology.kubernetes.io/zone\": \"us-central1-f\" \"topology.kubernetes.io/zone\": \"us-central1-c\" \"topology.kubernetes.io/zone\": \"us-central1-c\" \"topology.kubernetes.io/zone\": \"us-central1-a\" \"topology.kubernetes.io/zone\": \"us-central1-a\" cat pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc-tap labels: storageos.com/topology-aware: 'true' storageos.com/failure-mode: 'soft' spec: storageClassName: storageos accessModes: - ReadWriteOnce resources: requests: storage: 5Gi kubectl apply -f pvc.yaml persistentvolumeclaim/pvc-tap created kubectl get pvc pvc-tap -owide NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE VOLUMEMODE pvc-tap Bound pvc-03b55134-95b1-4609-a07b-3f73f115d0a3 5Gi RWO storageos 6m18s Filesystem kubectl describe pvc pvc-tap Name: pvc-tap Namespace: default StorageClass: storageos Status: Bound Volume: pvc-03b55134-95b1-4609-a07b-3f73f115d0a3 Labels: storageos.com/failure-mode=soft storageos.com/topology-aware=true Annotations: pv.kubernetes.io/bind-completed: yes pv.kubernetes.io/bound-by-controller: yes storageos.com/storageclass: 27550a36-cf0f-489e-8169-bab05ceb1cc7 volume.beta.kubernetes.io/storage-provisioner: csi.storageos.com Finalizers: [kubernetes.io/pvc-protection] Capacity: 5Gi Access Modes: RWO VolumeMode: Filesystem Used By: <none> Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Provisioning 19s csi.storageos.com_storageos-csi-helper-65dc8ff9d8-hmz4q_e65aa0d3-0541-4901-9e07-5898677577b1 External provisioner is provisioning volume for claim \"default/pvc-tap\" Normal ExternalProvisioning 19s persistentvolume-controller waiting for a volume to be created, either by external provisioner \"csi.storageos.com\" or manually created by system administrator Normal ProvisioningSucceeded 18s csi.storageos.com_storageos-csi-helper-65dc8ff9d8-hmz4q_e65aa0d3-0541-4901-9e07-5898677577b1 Successfully provisioned volume pvc-03b55134-95b1-4609-a07b-3f73f115d0a3 kubectl describe pv pvc-03b55134-95b1-4609-a07b-3f73f115d0a3 Name: pvc-03b55134-95b1-4609-a07b-3f73f115d0a3 Labels: <none> Annotations: pv.kubernetes.io/provisioned-by: csi.storageos.com Finalizers: [kubernetes.io/pv-protection] StorageClass: storageos Status: Bound Claim: default/pvc-tap Reclaim Policy: Delete Access Modes: RWO VolumeMode: Filesystem Capacity: 5Gi Node Affinity: <none> Message: Source: Type: CSI (a Container Storage Interface (CSI) volume source) Driver: csi.storageos.com FSType: ext4 VolumeHandle: 3ab351c2-0f75-4bab-8851-cf7c414811d5/4d65dddd-8109-4255-9639-edde066ab50d ReadOnly: false VolumeAttributes: csi.storage.k8s.io/pv/name=pvc-03b55134-95b1-4609-a07b-3f73f115d0a3 csi.storage.k8s.io/pvc/name=pvc-tap csi.storage.k8s.io/pvc/namespace=default storage.kubernetes.io/csiProvisionerIdentity=1636678069021-8081-csi.storageos.com storageos.com/failure-mode=soft storageos.com/nocompress=true storageos.com/topology-aware=true Events: <none> kubectl label pvc pvc-tap storageos.com/replicas=2 persistentvolumeclaim/pvc-tap labeled romuald_vandepoel@cloudshell:~/tap (shaped-complex-318513)$ kubectl describe pvc pvc-tapName: pvc-tap Namespace: default StorageClass: storageos Status: Bound Volume: pvc-03b55134-95b1-4609-a07b-3f73f115d0a3 Labels: storageos.com/failure-mode=soft storageos.com/replicas=2 storageos.com/topology-aware=true Annotations: pv.kubernetes.io/bind-completed: yes pv.kubernetes.io/bound-by-controller: yes storageos.com/storageclass: 27550a36-cf0f-489e-8169-bab05ceb1cc7 volume.beta.kubernetes.io/storage-provisioner: csi.storageos.com Finalizers: [kubernetes.io/pvc-protection] Capacity: 5Gi Access Modes: RWO VolumeMode: Filesystem Used By: <none> storageos describe volumes pvc-03b55134-95b1-4609-a07b-3f73f115d0a3 ID 4d65dddd-8109-4255-9639-edde066ab50d Name pvc-03b55134-95b1-4609-a07b-3f73f115d0a3 Description AttachedOn Attachment Type detached NFS Service Endpoint Exports: Namespace default (3ab351c2-0f75-4bab-8851-cf7c414811d5) Labels csi.storage.k8s.io/pv/name=pvc-03b55134-95b1-4609-a07b-3f73f115d0a3, csi.storage.k8s.io/pvc/name=pvc-tap, csi.storage.k8s.io/pvc/namespace=default, storageos.com/failure-mode=soft, storageos.com/nocompress=true, storageos.com/replicas=2, storageos.com/topology-aware=true Filesystem ext4 Size 5.0 GiB (5368709120 bytes) Version Nw Created at 2021-11-12T01:07:08Z (16 minutes ago) Updated at 2021-11-12T01:12:09Z (11 minutes ago) Master: ID d627cf2e-e4d6-4b2b-a595-45f3334762cf Node gke-cluster-1-default-pool-c6f6c5c1-ctcj (08d37e34-81c0-45a7-8507-355deeac9346) Health online Replicas: ID cd47db1f-ff00-4dfd-afd9-5ee06e1520ec Node gke-cluster-1-default-pool-9aba0152-hg08 (00c68f1c-2289-468e-a394-c60049936f58) Health ready Promotable true ID e9d61756-a0a2-403e-a38e-cdccc7ddb02c Node gke-cluster-1-default-pool-d81d0b14-9vfn (d7500964-ae30-451e-bcf2-93f8bd77a3bf) Health ready Promotable true kubectl get node {gke-cluster-1-default-pool-c6f6c5c1-ctcj,gke-cluster-1-default-pool-9aba0152-hg08,gke-cluster-1-default-pool-d81d0b14-9vfn} -ojson |jq '.items[].metadata.labels'|grep \"topology.kubernetes.io/zone\" \"topology.kubernetes.io/zone\": \"us-central1-c\" \"topology.kubernetes.io/zone\": \"us-central1-f\" \"topology.kubernetes.io/zone\": \"us-central1-a\" Done! And now, it would be great to \"kill\" a zone to perform disaster and resilience testing... but your turn! More... More! We want more! Well, our Intrepid Squirrel is not out of treats! There are more features and improvements to investigate. Visit our Docs website to get a deeper look at our Release Notes and let's hope the marketing department takes the hint and starts to use cool code names for releases!!!","title":"The Intrepid Squirrel is Out!"},{"location":"articles/the_intrepid_squirrel_is_out/#the-intrepid-squirrel-is-out","text":"Source: Ondat.io - Date: 8/12/2021 - Author: Romuald Vandepoel The Marketing and Product teams gave me a unique opportunity to write a blog post about the General Availability of Ondat 2.5. This is quite an honor and I will use it to raise my case regarding the usage of cool code names! Today, we are proud to announce the General Availability of 'Intrepid Squirrel' running on the 2.5 branch of our Ondat core product. Our Intrepid Squirrel is the result of our incredible Engineering teams, your work has been crucial in achieving this important milestone! You rock! Releasing at this time of the year is perfect to provide features and improvements that help with the holiday season operational freeze. Our little adventurous friend gathered the best treats to relieve our customers not only from the seasonal challenges but from the general operational burden of running cloud and on-prem Kubernetes clusters. What's in the tree dens? Our little friend's home is overflowing with good stuff! Here are my three favorite ones:","title":"The Intrepid Squirrel is Out!"},{"location":"articles/the_intrepid_squirrel_is_out/#i-am-tank-ill-be-your-operator","text":"It always pays to plan ahead and our Intrepid Squirrel knows it! This is the reason why our Operator has been entirely rewritten to pave the road to the future. Kubernetes is a fast-moving project with continuous improvements at every layer including the persistent storage one. The new operator provides our customer with a frictionless deployment and maintainability strategy, an agile adoption of new Kubernetes primitives while avoiding the deprecation traps, and an open door to new Ondat products!","title":"\"I am Tank. I'll be your operator\""},{"location":"articles/the_intrepid_squirrel_is_out/#happy-kubecuddle","text":"Exploring and maintaining a new solution is not always as simple as it looks despite well-written documentation. That's why our adventurous squirrel decided to streamline the installation and operational processes by introducing a kubectl plugin to deploy and operate one or many Ondat data mesh clusters. Let's have a couple of examples: Always good to have some preflight checks before having the \"hum!\" moment: kubectl storageos preflight Screenshot 2021-12-08 at 18.46.26 What about deploying a self-evaluation or test environment with one easy command: kubectl storageos install --include-etcd --admin-username rom --admin-password mysecretpassword namespace/storageos-etcd created kubectl get pod -n storageos-etcd NAME READY STATUS RESTARTS AGE storageos-etcd-0-qcq8h 1/1 Running 0 2m3s storageos-etcd-1-d7lbv 1/1 Running 0 2m3s storageos-etcd-2-p72tm 1/1 Running 0 2m3sstorageos-etcd-controller-manager-7c6df47dfb-5xlq5 1/1 Running 0 2m15s storageos-etcd-proxy-64cf4f6556-dnfrg 1/1 Running 0 2m15s kubectl get pod -n storageos NAME READY STATUS RESTARTS AGE storageos-api-manager-65f5c9dbdf-p4c92 1/1 Running 1 (52s ago) 61s storageos-api-manager-65f5c9dbdf-wdg59 1/1 Running 0 61s storageos-csi-helper-65dc8ff9d8-hmz4q 3/3 Running 0 61s storageos-node-6z4zd 3/3 Running 0 103s storageos-node-bvmrq 3/3 Running 0 103s storageos-node-czbp9 3/3 Running 0 103s storageos-node-fsqms 3/3 Running 0 103s storageos-node-tmrm8 3/3 Running 0 103s storageos-node-vbmml 3/3 Running 0 103s storageos-operator-b7f6ff69-qrkwv 2/2 Running 0 2m17sstorageos-scheduler-6f86686756-wpvzf 1/1 Running 0 109s Done! And there are more to look at... but your turn now!","title":"Happy Kubecuddle!"},{"location":"articles/the_intrepid_squirrel_is_out/#replicas-replicas-everywhere","text":"Whilst the cloud can abstract and simplify many infrastructure challenges, running a Kubernetes data plane in only one availability zone will result in a major disruption in event of a human or provider failure. Well in this release, our Intrepid Squirrel decided to extend his data home range with more than one availability zone, by detecting them with the native Kubernetes topology key or using our own custom key. Allowing the system to then provision the primary volumes and replicas evenly across these availability zones. In the event of a zone failure, our Rapid Recovery will elect one of the replicas as primary volume, call on the Kubernetes scheduler to restart the StatefulSet to the new zone, and start a new replica to honor the required replica count. Also with some clever optimizations, our Intrepid Squirrel got faster, way faster, up to 45 times faster when provisioning new replicas and rejoining a volume group by leveraging concurrent data replications across multiple zones and optimizing the network resources. Let's verify the available regions, create a PVC, scale the number of replicas to 2 (1 primary volume + 2 replicas = 3 zones), then check where are the volume and its replicas placed, hopefully in different availability zones: kubectl get node -ojson |jq '.items[].metadata.labels'|grep \"topology.kubernetes.io/zone\" \"topology.kubernetes.io/zone\": \"us-central1-f\" \"topology.kubernetes.io/zone\": \"us-central1-f\" \"topology.kubernetes.io/zone\": \"us-central1-c\" \"topology.kubernetes.io/zone\": \"us-central1-c\" \"topology.kubernetes.io/zone\": \"us-central1-a\" \"topology.kubernetes.io/zone\": \"us-central1-a\" cat pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc-tap labels: storageos.com/topology-aware: 'true' storageos.com/failure-mode: 'soft' spec: storageClassName: storageos accessModes: - ReadWriteOnce resources: requests: storage: 5Gi kubectl apply -f pvc.yaml persistentvolumeclaim/pvc-tap created kubectl get pvc pvc-tap -owide NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE VOLUMEMODE pvc-tap Bound pvc-03b55134-95b1-4609-a07b-3f73f115d0a3 5Gi RWO storageos 6m18s Filesystem kubectl describe pvc pvc-tap Name: pvc-tap Namespace: default StorageClass: storageos Status: Bound Volume: pvc-03b55134-95b1-4609-a07b-3f73f115d0a3 Labels: storageos.com/failure-mode=soft storageos.com/topology-aware=true Annotations: pv.kubernetes.io/bind-completed: yes pv.kubernetes.io/bound-by-controller: yes storageos.com/storageclass: 27550a36-cf0f-489e-8169-bab05ceb1cc7 volume.beta.kubernetes.io/storage-provisioner: csi.storageos.com Finalizers: [kubernetes.io/pvc-protection] Capacity: 5Gi Access Modes: RWO VolumeMode: Filesystem Used By: <none> Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Provisioning 19s csi.storageos.com_storageos-csi-helper-65dc8ff9d8-hmz4q_e65aa0d3-0541-4901-9e07-5898677577b1 External provisioner is provisioning volume for claim \"default/pvc-tap\" Normal ExternalProvisioning 19s persistentvolume-controller waiting for a volume to be created, either by external provisioner \"csi.storageos.com\" or manually created by system administrator Normal ProvisioningSucceeded 18s csi.storageos.com_storageos-csi-helper-65dc8ff9d8-hmz4q_e65aa0d3-0541-4901-9e07-5898677577b1 Successfully provisioned volume pvc-03b55134-95b1-4609-a07b-3f73f115d0a3 kubectl describe pv pvc-03b55134-95b1-4609-a07b-3f73f115d0a3 Name: pvc-03b55134-95b1-4609-a07b-3f73f115d0a3 Labels: <none> Annotations: pv.kubernetes.io/provisioned-by: csi.storageos.com Finalizers: [kubernetes.io/pv-protection] StorageClass: storageos Status: Bound Claim: default/pvc-tap Reclaim Policy: Delete Access Modes: RWO VolumeMode: Filesystem Capacity: 5Gi Node Affinity: <none> Message: Source: Type: CSI (a Container Storage Interface (CSI) volume source) Driver: csi.storageos.com FSType: ext4 VolumeHandle: 3ab351c2-0f75-4bab-8851-cf7c414811d5/4d65dddd-8109-4255-9639-edde066ab50d ReadOnly: false VolumeAttributes: csi.storage.k8s.io/pv/name=pvc-03b55134-95b1-4609-a07b-3f73f115d0a3 csi.storage.k8s.io/pvc/name=pvc-tap csi.storage.k8s.io/pvc/namespace=default storage.kubernetes.io/csiProvisionerIdentity=1636678069021-8081-csi.storageos.com storageos.com/failure-mode=soft storageos.com/nocompress=true storageos.com/topology-aware=true Events: <none> kubectl label pvc pvc-tap storageos.com/replicas=2 persistentvolumeclaim/pvc-tap labeled romuald_vandepoel@cloudshell:~/tap (shaped-complex-318513)$ kubectl describe pvc pvc-tapName: pvc-tap Namespace: default StorageClass: storageos Status: Bound Volume: pvc-03b55134-95b1-4609-a07b-3f73f115d0a3 Labels: storageos.com/failure-mode=soft storageos.com/replicas=2 storageos.com/topology-aware=true Annotations: pv.kubernetes.io/bind-completed: yes pv.kubernetes.io/bound-by-controller: yes storageos.com/storageclass: 27550a36-cf0f-489e-8169-bab05ceb1cc7 volume.beta.kubernetes.io/storage-provisioner: csi.storageos.com Finalizers: [kubernetes.io/pvc-protection] Capacity: 5Gi Access Modes: RWO VolumeMode: Filesystem Used By: <none> storageos describe volumes pvc-03b55134-95b1-4609-a07b-3f73f115d0a3 ID 4d65dddd-8109-4255-9639-edde066ab50d Name pvc-03b55134-95b1-4609-a07b-3f73f115d0a3 Description AttachedOn Attachment Type detached NFS Service Endpoint Exports: Namespace default (3ab351c2-0f75-4bab-8851-cf7c414811d5) Labels csi.storage.k8s.io/pv/name=pvc-03b55134-95b1-4609-a07b-3f73f115d0a3, csi.storage.k8s.io/pvc/name=pvc-tap, csi.storage.k8s.io/pvc/namespace=default, storageos.com/failure-mode=soft, storageos.com/nocompress=true, storageos.com/replicas=2, storageos.com/topology-aware=true Filesystem ext4 Size 5.0 GiB (5368709120 bytes) Version Nw Created at 2021-11-12T01:07:08Z (16 minutes ago) Updated at 2021-11-12T01:12:09Z (11 minutes ago) Master: ID d627cf2e-e4d6-4b2b-a595-45f3334762cf Node gke-cluster-1-default-pool-c6f6c5c1-ctcj (08d37e34-81c0-45a7-8507-355deeac9346) Health online Replicas: ID cd47db1f-ff00-4dfd-afd9-5ee06e1520ec Node gke-cluster-1-default-pool-9aba0152-hg08 (00c68f1c-2289-468e-a394-c60049936f58) Health ready Promotable true ID e9d61756-a0a2-403e-a38e-cdccc7ddb02c Node gke-cluster-1-default-pool-d81d0b14-9vfn (d7500964-ae30-451e-bcf2-93f8bd77a3bf) Health ready Promotable true kubectl get node {gke-cluster-1-default-pool-c6f6c5c1-ctcj,gke-cluster-1-default-pool-9aba0152-hg08,gke-cluster-1-default-pool-d81d0b14-9vfn} -ojson |jq '.items[].metadata.labels'|grep \"topology.kubernetes.io/zone\" \"topology.kubernetes.io/zone\": \"us-central1-c\" \"topology.kubernetes.io/zone\": \"us-central1-f\" \"topology.kubernetes.io/zone\": \"us-central1-a\" Done! And now, it would be great to \"kill\" a zone to perform disaster and resilience testing... but your turn!","title":"Replicas... replicas everywhere!"},{"location":"articles/the_intrepid_squirrel_is_out/#more-more-we-want-more","text":"Well, our Intrepid Squirrel is not out of treats! There are more features and improvements to investigate. Visit our Docs website to get a deeper look at our Release Notes and let's hope the marketing department takes the hint and starts to use cool code names for releases!!!","title":"More... More! We want more!"}]}